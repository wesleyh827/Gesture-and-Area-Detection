import cv2
from ultralytics import YOLO
import time
import numpy as np
import os

class AreaPersonDetector:
    def __init__(self, model_size='n', stay_duration=3.0):
        """
        åˆå§‹åŒ–å€åŸŸäººå“¡åµæ¸¬å™¨
        
        Args:
            model_size: æ¨¡å‹å¤§å° ('n', 's', 'm', 'l', 'x')
            stay_duration: éœ€è¦åœç•™çš„æ™‚é–“ï¼ˆç§’ï¼‰
        """
        print(f"è¼‰å…¥ YOLOv8{model_size} æ¨¡å‹...")
        self.model = YOLO(f'yolov8{model_size}.pt')  # ä½¿ç”¨ç‰©ä»¶åµæ¸¬æ¨¡å‹
        self.stay_threshold = stay_duration
        
        # åµæ¸¬å€åŸŸï¼ˆåˆå§‹ç‚º Noneï¼Œéœ€è¦ç”¨æˆ¶è¨­å®šï¼‰
        self.detection_area = None
        
        # è¿½è¹¤æ¯å€‹äººçš„è³‡è¨Š
        self.tracked_people = {}  # {person_id: {'enter_time': timestamp, 'counted': bool}}
        self.total_count = 0  # ç¸½è¨ˆæ•¸
        self.next_person_id = 1
        
        print(f"âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ")
        print(f"è¨­å®šï¼šéœ€åœ¨å€åŸŸå…§åœç•™ {stay_duration} ç§’æ‰è¨ˆæ•¸")
    
    def set_detection_area(self, video_path):
        """
        è®“ä½¿ç”¨è€…åœ¨å½±ç‰‡ç¬¬ä¸€å¹€ä¸Šæ¡†é¸åµæ¸¬å€åŸŸ
        
        Args:
            video_path: å½±ç‰‡è·¯å¾‘
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸè¨­å®šå€åŸŸ
        """
        cap = cv2.VideoCapture(video_path)
        ret, frame = cap.read()
        cap.release()
        
        if not ret:
            print("âŒ ç„¡æ³•è®€å–å½±ç‰‡ç¬¬ä¸€å¹€")
            return False
        
        print("\nğŸ“ è«‹åœ¨ç•«é¢ä¸Šæ¡†é¸åµæ¸¬å€åŸŸï¼š")
        print("   1. ç”¨æ»‘é¼ æ‹–æ›³æ¡†é¸çŸ©å½¢å€åŸŸ")
        print("   2. æŒ‰ 'r' é‡æ–°æ¡†é¸")
        print("   3. æŒ‰ 'c' ç¢ºèªå€åŸŸ")
        print("   4. æŒ‰ 'q' å–æ¶ˆ\n")
        
        # ä½¿ç”¨ OpenCV çš„ selectROI
        cv2.namedWindow('Select Detection Area', cv2.WINDOW_NORMAL)
        roi = cv2.selectROI('Select Detection Area', frame, fromCenter=False, showCrosshair=True)
        cv2.destroyAllWindows()
        
        if roi[2] > 0 and roi[3] > 0:  # æª¢æŸ¥æ˜¯å¦æœ‰æ•ˆ
            self.detection_area = {
                'x': int(roi[0]),
                'y': int(roi[1]),
                'width': int(roi[2]),
                'height': int(roi[3])
            }
            print(f"âœ“ åµæ¸¬å€åŸŸå·²è¨­å®š: ({self.detection_area['x']}, {self.detection_area['y']}) "
                  f"å¯¬={self.detection_area['width']}, é«˜={self.detection_area['height']}")
            return True
        else:
            print("âŒ æœªé¸æ“‡æœ‰æ•ˆå€åŸŸ")
            return False
    
    def is_person_in_area(self, bbox):
        """
        åˆ¤æ–·äººæ˜¯å¦åœ¨åµæ¸¬å€åŸŸå…§
        
        Args:
            bbox: [x1, y1, x2, y2] äººçš„é‚Šç•Œæ¡†
        
        Returns:
            bool: æ˜¯å¦åœ¨å€åŸŸå…§
        """
        if self.detection_area is None:
            return False
        
        # è¨ˆç®—äººçš„ä¸­å¿ƒé»
        person_center_x = (bbox[0] + bbox[2]) / 2
        person_center_y = (bbox[1] + bbox[3]) / 2
        
        # æª¢æŸ¥ä¸­å¿ƒé»æ˜¯å¦åœ¨å€åŸŸå…§
        area_x1 = self.detection_area['x']
        area_y1 = self.detection_area['y']
        area_x2 = area_x1 + self.detection_area['width']
        area_y2 = area_y1 + self.detection_area['height']
        
        return (area_x1 <= person_center_x <= area_x2 and 
                area_y1 <= person_center_y <= area_y2)
    
    def get_person_id(self, bbox, current_time):
        """
        ç°¡å–®çš„äººå“¡è¿½è¹¤ï¼ˆæ ¹æ“šä½ç½®ç›¸è¿‘åº¦ï¼‰
        
        Args:
            bbox: [x1, y1, x2, y2] äººçš„é‚Šç•Œæ¡†
            current_time: ç•¶å‰æ™‚é–“æˆ³
        
        Returns:
            int: äººå“¡ID
        """
        center_x = (bbox[0] + bbox[2]) / 2
        center_y = (bbox[1] + bbox[3]) / 2
        
        # å°‹æ‰¾æœ€æ¥è¿‘çš„å·²è¿½è¹¤äººå“¡ï¼ˆç°¡å–®çš„è¿½è¹¤é‚è¼¯ï¼‰
        min_distance = float('inf')
        matched_id = None
        
        for pid, info in self.tracked_people.items():
            if 'last_position' in info:
                last_x, last_y = info['last_position']
                distance = np.sqrt((center_x - last_x)**2 + (center_y - last_y)**2)
                
                # å¦‚æœè·é›¢å°æ–¼é–¾å€¼ï¼Œèªç‚ºæ˜¯åŒä¸€å€‹äºº
                if distance < 100 and distance < min_distance:
                    min_distance = distance
                    matched_id = pid
        
        if matched_id is None:
            # æ–°çš„äºº
            matched_id = self.next_person_id
            self.next_person_id += 1
            self.tracked_people[matched_id] = {
                'enter_time': None,
                'counted': False,
                'last_seen': current_time
            }
        
        # æ›´æ–°ä½ç½®å’Œæœ€å¾Œå‡ºç¾æ™‚é–“
        self.tracked_people[matched_id]['last_position'] = (center_x, center_y)
        self.tracked_people[matched_id]['last_seen'] = current_time
        
        return matched_id
    
    def clean_old_tracks(self, current_time, timeout=2.0):
        """æ¸…é™¤è¶…éä¸€å®šæ™‚é–“æœªå‡ºç¾çš„è¿½è¹¤"""
        to_remove = []
        for pid, info in self.tracked_people.items():
            if current_time - info['last_seen'] > timeout:
                to_remove.append(pid)
        
        for pid in to_remove:
            del self.tracked_people[pid]
    
    def draw_detection_area(self, frame):
        """åœ¨ç•«é¢ä¸Šç¹ªè£½åµæ¸¬å€åŸŸ"""
        if self.detection_area is None:
            return frame
        
        x = self.detection_area['x']
        y = self.detection_area['y']
        w = self.detection_area['width']
        h = self.detection_area['height']
        
        # ç¹ªè£½åŠé€æ˜å€åŸŸ
        overlay = frame.copy()
        cv2.rectangle(overlay, (x, y), (x + w, y + h), (0, 255, 0), -1)
        cv2.addWeighted(overlay, 0.2, frame, 0.8, 0, frame)
        
        # ç¹ªè£½é‚Šæ¡†
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)
        
        # ç¹ªè£½å€åŸŸæ¨™ç±¤
        cv2.putText(frame, "Detection Area", (x + 5, y + 25),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        
        return frame
    
    def draw_info(self, frame):
        """ç¹ªè£½çµ±è¨ˆè³‡è¨Š"""
        # èƒŒæ™¯æ¡†
        cv2.rectangle(frame, (10, 10), (400, 120), (0, 0, 0), -1)
        
        # çµ±è¨ˆè³‡è¨Š
        cv2.putText(frame, f"Total Count: {self.total_count}", (20, 40),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # ç•¶å‰åœ¨å€åŸŸå…§çš„äººæ•¸
        current_in_area = sum(1 for info in self.tracked_people.values() 
                            if info['enter_time'] is not None)
        cv2.putText(frame, f"Current in Area: {current_in_area}", (20, 75),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
        
        # è¿½è¹¤ä¸­çš„äººæ•¸
        cv2.putText(frame, f"Tracking: {len(self.tracked_people)}", (20, 105),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        return frame
    
    def process_frame(self, frame, current_time):
        """
        è™•ç†å–®ä¸€å½±æ ¼
        
        Args:
            frame: è¼¸å…¥å½±æ ¼
            current_time: ç•¶å‰æ™‚é–“æˆ³
        
        Returns:
            annotated_frame: æ¨™è¨»å¾Œçš„å½±æ ¼
        """
        # åŸ·è¡Œäººå“¡åµæ¸¬
        results = self.model(frame, verbose=False)
        
        # ç¹ªè£½åµæ¸¬å€åŸŸ
        annotated_frame = self.draw_detection_area(frame.copy())
        
        # è™•ç†åµæ¸¬åˆ°çš„äºº
        if results[0].boxes is not None and len(results[0].boxes) > 0:
            boxes = results[0].boxes.xyxy.cpu().numpy()
            classes = results[0].boxes.cls.cpu().numpy()
            confidences = results[0].boxes.conf.cpu().numpy()
            
            for box, cls, conf in zip(boxes, classes, confidences):
                # åªè™•ç† person (class 0) ä¸”ä¿¡å¿ƒåº¦ > 0.5
                if int(cls) == 0 and conf > 0.5:
                    # å–å¾—æˆ–åˆ†é…äººå“¡ID
                    person_id = self.get_person_id(box, current_time)
                    
                    # åˆ¤æ–·æ˜¯å¦åœ¨å€åŸŸå…§
                    in_area = self.is_person_in_area(box)
                    
                    if in_area:
                        # åœ¨å€åŸŸå…§
                        if self.tracked_people[person_id]['enter_time'] is None:
                            # å‰›é€²å…¥å€åŸŸ
                            self.tracked_people[person_id]['enter_time'] = current_time
                            print(f"Person #{person_id} é€²å…¥å€åŸŸ")
                        
                        # è¨ˆç®—åœç•™æ™‚é–“
                        stay_duration = current_time - self.tracked_people[person_id]['enter_time']
                        
                        # æª¢æŸ¥æ˜¯å¦é”åˆ°è¨ˆæ•¸æ¢ä»¶
                        if stay_duration >= self.stay_threshold and not self.tracked_people[person_id]['counted']:
                            self.tracked_people[person_id]['counted'] = True
                            self.total_count += 1
                            print(f"âœ“ Person #{person_id} å·²è¨ˆæ•¸ï¼(åœç•™ {stay_duration:.1f}ç§’) - ç¸½è¨ˆ: {self.total_count}")
                        
                        # ç¹ªè£½é‚Šç•Œæ¡†ï¼ˆæ ¹æ“šç‹€æ…‹é¸æ“‡é¡è‰²ï¼‰
                        if self.tracked_people[person_id]['counted']:
                            color = (0, 255, 0)  # ç¶ è‰²ï¼šå·²è¨ˆæ•¸
                            status = "COUNTED"
                        else:
                            color = (0, 165, 255)  # æ©˜è‰²ï¼šè¨ˆæ™‚ä¸­
                            status = f"{stay_duration:.1f}s"
                    else:
                        # ä¸åœ¨å€åŸŸå…§
                        if self.tracked_people[person_id]['enter_time'] is not None:
                            # é›¢é–‹å€åŸŸï¼Œé‡ç½®
                            print(f"Person #{person_id} é›¢é–‹å€åŸŸ")
                            self.tracked_people[person_id]['enter_time'] = None
                            self.tracked_people[person_id]['counted'] = False
                        
                        color = (128, 128, 128)  # ç°è‰²ï¼šå€åŸŸå¤–
                        status = "Outside"
                    
                    # ç¹ªè£½é‚Šç•Œæ¡†
                    x1, y1, x2, y2 = map(int, box)
                    cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
                    
                    # ç¹ªè£½æ¨™ç±¤
                    label = f"P{person_id} [{status}]"
                    cv2.putText(annotated_frame, label, (x1, y1 - 10),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # æ¸…ç†èˆŠçš„è¿½è¹¤
        self.clean_old_tracks(current_time)
        
        # ç¹ªè£½çµ±è¨ˆè³‡è¨Š
        annotated_frame = self.draw_info(annotated_frame)
        
        return annotated_frame
    
    def process_video(self, video_path, save_output=True, output_dir='output', show_video=True):
        """
        è™•ç†å½±ç‰‡
        
        Args:
            video_path: å½±ç‰‡è·¯å¾‘
            save_output: æ˜¯å¦å„²å­˜çµæœ
            output_dir: è¼¸å‡ºè³‡æ–™å¤¾
            show_video: æ˜¯å¦é¡¯ç¤ºå³æ™‚ç•«é¢
        """
        print(f"\nè™•ç†å½±ç‰‡: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡")
            return
        
        # å–å¾—å½±ç‰‡è³‡è¨Š
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"å½±ç‰‡è³‡è¨Š: {width}x{height} @ {fps}fps, ç¸½é•·åº¦: {total_frames/fps/60:.1f} åˆ†é˜")
        
        # æº–å‚™è¼¸å‡º
        out = None
        output_path = None
        if save_output:
            os.makedirs(output_dir, exist_ok=True)
            filename = os.path.basename(video_path)
            name, ext = os.path.splitext(filename)
            output_path = os.path.join(output_dir, f"{name}_area_result{ext}")
            
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            print(f"ğŸ’¾ å°‡å„²å­˜çµæœè‡³: {output_path}")
        
        print("\né–‹å§‹è™•ç†...")
        if show_video:
            print("æŒ‰ 'q' çµæŸï¼ŒæŒ‰ 'p' æš«åœ")
        
        frame_count = 0
        start_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            current_time = frame_count / fps
            
            # è™•ç†å½±æ ¼
            processed_frame = self.process_frame(frame, current_time)
            
            # å„²å­˜
            if out is not None:
                out.write(processed_frame)
            
            # é¡¯ç¤ºé€²åº¦
            if frame_count % 30 == 0:
                progress = (frame_count / total_frames) * 100
                elapsed = time.time() - start_time
                fps_actual = frame_count / elapsed
                eta = (total_frames - frame_count) / fps_actual / 60
                print(f"\ré€²åº¦: {progress:.1f}% | é€Ÿåº¦: {fps_actual:.1f} fps | "
                      f"å‰©é¤˜: {eta:.1f} åˆ† | è¨ˆæ•¸: {self.total_count}", end='')
            
            # é¡¯ç¤ºç•«é¢
            if show_video:
                cv2.imshow('Area Person Detection', processed_frame)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("\n\nâš  ä½¿ç”¨è€…ä¸­æ–·")
                    break
                elif key == ord('p'):
                    print("\næš«åœ...")
                    cv2.waitKey(0)
        
        # æ¸…ç†
        cap.release()
        if out is not None:
            out.release()
        cv2.destroyAllWindows()
        
        # é¡¯ç¤ºçµæœ
        print(f"\n\n{'='*60}")
        print(f"è™•ç†å®Œæˆï¼")
        print(f"ç¸½å½±æ ¼æ•¸: {frame_count}")
        print(f"è™•ç†æ™‚é–“: {(time.time() - start_time)/60:.1f} åˆ†é˜")
        print(f"âœ“ åœ¨å€åŸŸå…§åœç•™è¶…é {self.stay_threshold} ç§’çš„äººæ•¸: {self.total_count}")
        
        if save_output and output_path:
            print(f"\nğŸ’¾ çµæœå·²å„²å­˜è‡³: {output_path}")
        
        print(f"{'='*60}\n")


def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 60)
    print("å€åŸŸäººå“¡åœç•™åµæ¸¬ç³»çµ±")
    print("=" * 60)
    
    # å–å¾—å½±ç‰‡è·¯å¾‘
    video_path = input("\nè«‹è¼¸å…¥å½±ç‰‡è·¯å¾‘: ").strip()
    
    if not os.path.exists(video_path):
        print("âŒ æ‰¾ä¸åˆ°å½±ç‰‡æª”æ¡ˆ")
        return
    
    # è¨­å®šåœç•™æ™‚é–“
    stay_duration = input("è«‹è¼¸å…¥éœ€è¦åœç•™çš„ç§’æ•¸ (é è¨­: 3): ").strip()
    try:
        stay_duration = float(stay_duration) if stay_duration else 3.0
    except:
        stay_duration = 3.0
    
    # å»ºç«‹åµæ¸¬å™¨
    detector = AreaPersonDetector(model_size='n', stay_duration=stay_duration)
    
    # è¨­å®šåµæ¸¬å€åŸŸ
    print("\næ­£åœ¨é–‹å•Ÿå½±ç‰‡ä»¥è¨­å®šåµæ¸¬å€åŸŸ...")
    if not detector.set_detection_area(video_path):
        print("âŒ æœªè¨­å®šåµæ¸¬å€åŸŸï¼Œç¨‹å¼çµæŸ")
        return
    
    # é¸æ“‡æ˜¯å¦å„²å­˜
    save = input("\næ˜¯å¦å„²å­˜çµæœå½±ç‰‡? (y/n, é è¨­:y): ").strip().lower()
    save_output = save != 'n'
    
    # é¸æ“‡æ˜¯å¦é¡¯ç¤ºå³æ™‚ç•«é¢
    show = input("æ˜¯å¦é¡¯ç¤ºå³æ™‚è™•ç†ç•«é¢? (y/n, é è¨­:y): ").strip().lower()
    show_video = show != 'n'
    
    # é–‹å§‹è™•ç†
    detector.process_video(video_path, save_output=save_output, show_video=show_video)


if __name__ == "__main__":
    main()
import cv2
from ultralytics import YOLO
import time
import numpy as np
import os

class GestureDetector:
    def __init__(self, model_size='n', alert_duration=3.0):
        """
        åˆå§‹åŒ–æ‰‹å‹¢åµæ¸¬å™¨
        
        Args:
            model_size: æ¨¡å‹å¤§å° ('n', 's', 'm', 'l', 'x')
            alert_duration: è­¦ç¤ºå‹•ä½œæŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰
        """
        print(f"è¼‰å…¥ YOLOv8-{model_size}-pose æ¨¡å‹...")
        self.model = YOLO(f'yolov8{model_size}-pose.pt')
        self.alert_threshold = alert_duration
        self.alert_start_time = None
        self.is_alerting = False
        self.gui_available = self._check_gui_support()
        
    def _check_gui_support(self):
        """æª¢æŸ¥æ˜¯å¦æ”¯æ´ GUI é¡¯ç¤º"""
        try:
            # å˜—è©¦å‰µå»ºä¸€å€‹æ¸¬è©¦è¦–çª—
            test_img = np.zeros((100, 100, 3), dtype=np.uint8)
            cv2.imshow('test', test_img)
            cv2.destroyAllWindows()
            return True
        except:
            print("âš  åµæ¸¬åˆ°ç³»çµ±ä¸æ”¯æ´ GUI é¡¯ç¤ºï¼Œå°‡åªå„²å­˜çµæœåœ–ç‰‡")
            return False
        
    def check_hands_above_shoulders(self, keypoints):
        """
        æª¢æŸ¥é›™æ‰‹æ˜¯å¦éƒ½åœ¨è‚©è†€ä»¥ä¸Šï¼ˆæˆ–æ¥è¿‘ï¼‰
        
        Args:
            keypoints: [17, 3] é™£åˆ—ï¼ŒåŒ…å« (x, y, confidence)
        
        Returns:
            bool: é›™æ‰‹æ˜¯å¦åœ¨è‚©è†€ä»¥ä¸Š
        """
        # COCO keypoints ç´¢å¼•
        # 5: å·¦è‚©, 6: å³è‚©, 7: å·¦æ‰‹è‚˜, 8: å³æ‰‹è‚˜, 9: å·¦æ‰‹è…•, 10: å³æ‰‹è…•
        left_shoulder = keypoints[5]
        right_shoulder = keypoints[6]
        left_elbow = keypoints[7]
        right_elbow = keypoints[8]
        left_wrist = keypoints[9]
        right_wrist = keypoints[10]
        
        # æª¢æŸ¥confidenceï¼ˆä¿¡å¿ƒåº¦ï¼‰- é™ä½é–¾å€¼è®“åµæ¸¬æ›´å®¹æ˜“
        confidence_threshold = 0.4
        
        # è¨ˆç®—å¹³å‡è‚©è†€é«˜åº¦ï¼ˆyåº§æ¨™è¶Šå°è¶Šä¸Šæ–¹ï¼‰
        if left_shoulder[2] < confidence_threshold or right_shoulder[2] < confidence_threshold:
            return False
        
        shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2
        
        # å®¹å·®å€¼ï¼ˆåƒç´ ï¼‰- æ‰‹è…•æ¥è¿‘è‚©è†€ä¹Ÿç®—èˆ‰æ‰‹
        # èª¿æ•´é€™å€‹å€¼ä¾†æ”¹è®Šéˆæ•åº¦ï¼šæ•¸å­—è¶Šå¤§è¶Šå®¹æ˜“è§¸ç™¼
        tolerance = 60
        
        # å·¦æ‰‹åˆ¤å®šï¼šå„ªå…ˆç”¨æ‰‹è…•ï¼Œå¦‚æœæ‰‹è…•ä¿¡å¿ƒåº¦ä¸å¤ å‰‡ç”¨æ‰‹è‚˜
        left_hand_up = False
        if left_wrist[2] >= confidence_threshold:
            left_hand_up = left_wrist[1] < (shoulder_y + tolerance)
        elif left_elbow[2] >= confidence_threshold:
            # å¦‚æœæ‰‹è…•åµæ¸¬ä¸åˆ°ï¼Œæ‰‹è‚˜åœ¨è‚©è†€é™„è¿‘ä¹Ÿç®—
            left_hand_up = left_elbow[1] < (shoulder_y + tolerance * 0.5)
        
        # å³æ‰‹åˆ¤å®šï¼šåŒä¸Š
        right_hand_up = False
        if right_wrist[2] >= confidence_threshold:
            right_hand_up = right_wrist[1] < (shoulder_y + tolerance)
        elif right_elbow[2] >= confidence_threshold:
            right_hand_up = right_elbow[1] < (shoulder_y + tolerance * 0.5)
        
        return left_hand_up and right_hand_up
    
    def draw_info(self, frame, hands_up, duration=0, is_static=False):
        """åœ¨ç•«é¢ä¸Šç¹ªè£½è³‡è¨Š"""
        # ç¹ªè£½ç‹€æ…‹è³‡è¨Š
        if hands_up:
            if is_static or duration >= self.alert_threshold:
                # è­¦ç¤ºè§¸ç™¼ï¼ˆåœ–ç‰‡æ¨¡å¼æˆ–é”åˆ°æ™‚é–“ï¼‰
                cv2.rectangle(frame, (0, 0), (frame.shape[1], 100), (0, 0, 255), -1)
                cv2.putText(frame, "WARNING: ALERT POSE DETECTED!", (50, 60),
                          cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 255, 255), 4)
            else:
                # åµæ¸¬åˆ°èˆ‰æ‰‹ä½†æœªé”3ç§’
                cv2.putText(frame, f"Hands Up: {duration:.1f}s / {self.alert_threshold}s", 
                          (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
        else:
            # æœªåµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢
            if is_static:
                cv2.putText(frame, "No Alert Pose Detected", (20, 40),
                          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        return frame
    
    def process_frame(self, frame, is_static=False):
        """
        è™•ç†å–®ä¸€å½±æ ¼
        
        Args:
            frame: è¼¸å…¥å½±æ ¼
            is_static: æ˜¯å¦ç‚ºéœæ…‹åœ–ç‰‡ï¼ˆä¸éœ€è¨ˆæ™‚ï¼‰
        
        Returns:
            annotated_frame: æ¨™è¨»å¾Œçš„å½±æ ¼
            hands_up: æ˜¯å¦åµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢
        """
        # åŸ·è¡Œå§¿æ…‹ä¼°è¨ˆ
        results = self.model(frame, verbose=False)
        
        hands_up = False
        duration = 0
        
        # æª¢æŸ¥æ˜¯å¦æœ‰åµæ¸¬åˆ°äºº
        if results[0].keypoints is not None and len(results[0].keypoints.data) > 0:
            # åªè™•ç†ç¬¬ä¸€å€‹äººï¼ˆå¯æ“´å±•ç‚ºå¤šäººï¼‰
            person_keypoints = results[0].keypoints.data[0]
            
            # æª¢æŸ¥æ˜¯å¦é›™æ‰‹èˆ‰èµ·
            if self.check_hands_above_shoulders(person_keypoints):
                hands_up = True
                
                if not is_static:
                    if self.alert_start_time is None:
                        self.alert_start_time = time.time()
                    
                    duration = time.time() - self.alert_start_time
                    
                    # æª¢æŸ¥æ˜¯å¦é”åˆ°è­¦ç¤ºæ™‚é–“
                    if duration >= self.alert_threshold and not self.is_alerting:
                        self.is_alerting = True
                        print(f"\nğŸš¨ è­¦ç¤ºè§¸ç™¼ï¼æŒçºŒæ™‚é–“: {duration:.2f}ç§’")
            else:
                # é‡ç½®è¨ˆæ™‚å™¨
                if not is_static and self.alert_start_time is not None:
                    print(f"é‡ç½®è¨ˆæ™‚å™¨ï¼ˆæŒçºŒäº† {time.time() - self.alert_start_time:.2f}ç§’ï¼‰")
                self.alert_start_time = None
                self.is_alerting = False
        
        # ç¹ªè£½å§¿æ…‹éª¨æ¶
        annotated_frame = results[0].plot()
        
        # ç¹ªè£½è³‡è¨Š
        annotated_frame = self.draw_info(annotated_frame, hands_up, duration, is_static)
        
        return annotated_frame, hands_up
    
    def process_image(self, image_path, save_output=True, output_dir='output'):
        """
        è™•ç†å–®å¼µåœ–ç‰‡
        
        Args:
            image_path: åœ–ç‰‡è·¯å¾‘
            save_output: æ˜¯å¦å„²å­˜çµæœ
            output_dir: è¼¸å‡ºè³‡æ–™å¤¾
        
        Returns:
            bool: æ˜¯å¦åµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢
        """
        print(f"\nè™•ç†åœ–ç‰‡: {image_path}")
        
        # è®€å–åœ–ç‰‡
        if not os.path.exists(image_path):
            print(f"âŒ æ‰¾ä¸åˆ°åœ–ç‰‡: {image_path}")
            return False
        
        frame = cv2.imread(image_path)
        if frame is None:
            print(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡: {image_path}")
            return False
        
        # è™•ç†åœ–ç‰‡ï¼ˆéœæ…‹æ¨¡å¼ï¼‰
        annotated_frame, hands_up = self.process_frame(frame, is_static=True)
        
        # é¡¯ç¤ºçµæœ
        print(f"{'âœ… åµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢ï¼' if hands_up else 'âŒ æœªåµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢'}")
        
        # å„²å­˜çµæœ
        output_path = None
        if save_output:
            os.makedirs(output_dir, exist_ok=True)
            filename = os.path.basename(image_path)
            name, ext = os.path.splitext(filename)
            output_path = os.path.join(output_dir, f"{name}_result{ext}")
            cv2.imwrite(output_path, annotated_frame)
            print(f"ğŸ’¾ çµæœå·²å„²å­˜è‡³: {output_path}")
        
        # åªåœ¨æ”¯æ´ GUI æ™‚é¡¯ç¤ºåœ–ç‰‡
        if self.gui_available:
            try:
                cv2.imshow('Gesture Detection - Image', annotated_frame)
                print("æŒ‰ä»»æ„éµç¹¼çºŒ...")
                cv2.waitKey(0)
                cv2.destroyAllWindows()
            except Exception as e:
                print(f"âš  ç„¡æ³•é¡¯ç¤ºåœ–ç‰‡: {e}")
        else:
            print(f"ğŸ’¡ è«‹é–‹å•Ÿ {output_path} æŸ¥çœ‹çµæœ")
        
        return hands_up
    
    def process_image_batch(self, image_folder, save_output=True, output_dir='output'):
        """
        æ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡
        
        Args:
            image_folder: åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘
            save_output: æ˜¯å¦å„²å­˜çµæœ
            output_dir: è¼¸å‡ºè³‡æ–™å¤¾
        """
        print(f"\næ‰¹æ¬¡è™•ç†è³‡æ–™å¤¾: {image_folder}")
        
        if not os.path.exists(image_folder):
            print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {image_folder}")
            return
        
        # æ”¯æ´çš„åœ–ç‰‡æ ¼å¼
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.webp']
        
        # å–å¾—æ‰€æœ‰åœ–ç‰‡
        image_files = []
        for file in os.listdir(image_folder):
            if any(file.lower().endswith(ext) for ext in image_extensions):
                image_files.append(os.path.join(image_folder, file))
        
        if not image_files:
            print("âŒ è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡æª”æ¡ˆ")
            return
        
        print(f"æ‰¾åˆ° {len(image_files)} å¼µåœ–ç‰‡\n")
        
        # çµ±è¨ˆçµæœ
        alert_count = 0
        
        # è™•ç†æ¯å¼µåœ–ç‰‡
        for i, image_path in enumerate(image_files, 1):
            print(f"[{i}/{len(image_files)}] ", end="")
            
            frame = cv2.imread(image_path)
            if frame is None:
                print(f"âš  è·³éç„¡æ³•è®€å–çš„åœ–ç‰‡: {image_path}")
                continue
            
            # è™•ç†åœ–ç‰‡
            annotated_frame, hands_up = self.process_frame(frame, is_static=True)
            
            if hands_up:
                alert_count += 1
                print(f"âœ… {os.path.basename(image_path)} - åµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢")
            else:
                print(f"âŒ {os.path.basename(image_path)} - æœªåµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢")
            
            # å„²å­˜çµæœ
            if save_output:
                os.makedirs(output_dir, exist_ok=True)
                filename = os.path.basename(image_path)
                name, ext = os.path.splitext(filename)
                output_path = os.path.join(output_dir, f"{name}_result{ext}")
                cv2.imwrite(output_path, annotated_frame)
        
        # é¡¯ç¤ºçµ±è¨ˆ
        print(f"\n{'='*50}")
        print(f"è™•ç†å®Œæˆï¼")
        print(f"ç¸½åœ–ç‰‡æ•¸: {len(image_files)}")
        print(f"åµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢: {alert_count} å¼µ")
        print(f"æœªåµæ¸¬åˆ°: {len(image_files) - alert_count} å¼µ")
        if save_output:
            print(f"ğŸ’¾ çµæœå·²å„²å­˜è‡³: {output_dir}/")
        print(f"{'='*50}")
    
    def run_webcam(self, camera_id=0):
        """ä½¿ç”¨æ”å½±æ©Ÿå³æ™‚åµæ¸¬"""
        if not self.gui_available:
            print("âŒ ç³»çµ±ä¸æ”¯æ´ GUI é¡¯ç¤ºï¼Œç„¡æ³•ä½¿ç”¨æ”å½±æ©Ÿæ¨¡å¼")
            print("ğŸ’¡ è«‹ä½¿ç”¨åœ–ç‰‡è™•ç†æ¨¡å¼")
            return
            
        print(f"\né–‹å§‹ä½¿ç”¨æ”å½±æ©Ÿ {camera_id}")
        print("è«‹é›™æ‰‹èˆ‰éè‚©è†€3ç§’ä»¥ä¸Šä¾†æ¸¬è©¦")
        print("æŒ‰ 'q' é›¢é–‹\n")
        
        cap = cv2.VideoCapture(camera_id)
        
        if not cap.isOpened():
            print(f"âŒ ç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿ {camera_id}")
            return
        
        # è¨­å®šè§£æåº¦ï¼ˆå¯é¸ï¼‰
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                print("ç„¡æ³•è®€å–å½±åƒ")
                break
            
            # è™•ç†å½±æ ¼
            processed_frame, _ = self.process_frame(frame, is_static=False)
            
            # é¡¯ç¤ºçµæœ
            cv2.imshow('Gesture Detection - Webcam', processed_frame)
            
            # æŒ‰ 'q' é›¢é–‹
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        print("ç¨‹å¼çµæŸ")
    
    def run_video(self, video_path, save_output=True, output_dir='output'):
        """
        è™•ç†å½±ç‰‡æª”æ¡ˆ
        
        Args:
            video_path: å½±ç‰‡è·¯å¾‘
            save_output: æ˜¯å¦å„²å­˜çµæœå½±ç‰‡
            output_dir: è¼¸å‡ºè³‡æ–™å¤¾
        """
        print(f"\nè™•ç†å½±ç‰‡: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡ {video_path}")
            return
        
        # å–å¾—å½±ç‰‡è³‡è¨Š
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"å½±ç‰‡è³‡è¨Š: {width}x{height} @ {fps}fps, ç¸½å½±æ ¼æ•¸: {total_frames}")
        
        # æº–å‚™è¼¸å‡ºå½±ç‰‡
        out = None
        output_path = None
        if save_output:
            os.makedirs(output_dir, exist_ok=True)
            filename = os.path.basename(video_path)
            name, ext = os.path.splitext(filename)
            output_path = os.path.join(output_dir, f"{name}_result{ext}")
            
            # ä½¿ç”¨ mp4v ç·¨ç¢¼å™¨ï¼ˆç›¸å®¹æ€§è¼ƒå¥½ï¼‰
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            print(f"ğŸ’¾ å°‡å„²å­˜çµæœè‡³: {output_path}")
        
        # è¨˜éŒ„è­¦ç¤ºäº‹ä»¶
        alert_events = []
        frame_count = 0
        
        print("\né–‹å§‹è™•ç†å½±ç‰‡...")
        if self.gui_available:
            print("æŒ‰ 'q' æå‰çµæŸï¼ŒæŒ‰ 'p' æš«åœ")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # è™•ç†å½±æ ¼
            processed_frame, hands_up = self.process_frame(frame, is_static=False)
            
            # è¨˜éŒ„è­¦ç¤ºäº‹ä»¶
            if self.is_alerting and (not alert_events or not alert_events[-1]['end']):
                if not alert_events or alert_events[-1]['end']:
                    # æ–°çš„è­¦ç¤ºäº‹ä»¶
                    alert_events.append({
                        'start_frame': frame_count,
                        'start_time': frame_count / fps,
                        'end': False
                    })
            elif not self.is_alerting and alert_events and not alert_events[-1]['end']:
                # è­¦ç¤ºçµæŸ
                alert_events[-1]['end_frame'] = frame_count
                alert_events[-1]['end_time'] = frame_count / fps
                alert_events[-1]['end'] = True
            
            # å„²å­˜å½±æ ¼
            if out is not None:
                out.write(processed_frame)
            
            # é¡¯ç¤ºé€²åº¦
            if frame_count % 30 == 0:  # æ¯30å¹€é¡¯ç¤ºä¸€æ¬¡
                progress = (frame_count / total_frames) * 100
                print(f"\rè™•ç†ä¸­... {progress:.1f}% ({frame_count}/{total_frames})", end='')
            
            # é¡¯ç¤ºçµæœï¼ˆå¦‚æœæ”¯æ´GUIï¼‰
            if self.gui_available:
                cv2.imshow('Gesture Detection - Video', processed_frame)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("\n\nâš  ä½¿ç”¨è€…ä¸­æ–·è™•ç†")
                    break
                elif key == ord('p'):
                    print("\næš«åœä¸­ï¼ŒæŒ‰ä»»æ„éµç¹¼çºŒ...")
                    cv2.waitKey(0)
        
        print("\n\nå½±ç‰‡è™•ç†å®Œæˆï¼")
        
        # é‡‹æ”¾è³‡æº
        cap.release()
        if out is not None:
            out.release()
        if self.gui_available:
            cv2.destroyAllWindows()
        
        # é¡¯ç¤ºçµæœçµ±è¨ˆ
        print(f"\n{'='*50}")
        print(f"è™•ç†çµ±è¨ˆ:")
        print(f"ç¸½å½±æ ¼æ•¸: {frame_count}")
        print(f"å½±ç‰‡é•·åº¦: {frame_count/fps:.2f} ç§’")
        print(f"åµæ¸¬åˆ°è­¦ç¤ºäº‹ä»¶: {len(alert_events)} æ¬¡")
        
        if alert_events:
            print(f"\nè­¦ç¤ºäº‹ä»¶è©³æƒ…:")
            for i, event in enumerate(alert_events, 1):
                start_time = event['start_time']
                end_time = event.get('end_time', frame_count / fps)
                duration = end_time - start_time
                print(f"  äº‹ä»¶ {i}: {start_time:.2f}s - {end_time:.2f}s (æŒçºŒ {duration:.2f}s)")
        
        if save_output and output_path:
            print(f"\nğŸ’¾ çµæœå½±ç‰‡å·²å„²å­˜è‡³: {output_path}")
        
        print(f"{'='*50}\n")


def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 50)
    print("æ‰‹å‹¢è­¦ç¤ºåµæ¸¬ç³»çµ±")
    print("=" * 50)
    
    # å»ºç«‹åµæ¸¬å™¨
    detector = GestureDetector(
        model_size='n',  # ä½¿ç”¨ nano æ¨¡å‹ï¼ˆæœ€å¿«ï¼‰
        alert_duration=3.0  # 3ç§’è­¦ç¤º
    )
    
    # é¸æ“‡æ¨¡å¼
    print("\nè«‹é¸æ“‡æ¨¡å¼:")
    print("1. ä½¿ç”¨æ”å½±æ©Ÿå³æ™‚åµæ¸¬")
    print("2. è™•ç†å½±ç‰‡æª”æ¡ˆ")
    print("3. è™•ç†å–®å¼µåœ–ç‰‡")
    print("4. æ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡")
    
    choice = input("\nè«‹è¼¸å…¥é¸é … (1-4): ").strip()
    
    if choice == '1':
        # ä½¿ç”¨æ”å½±æ©Ÿ
        detector.run_webcam(camera_id=0)
    elif choice == '2':
        # ä½¿ç”¨å½±ç‰‡æª”æ¡ˆ
        video_path = input("è«‹è¼¸å…¥å½±ç‰‡è·¯å¾‘: ").strip()
        save = input("æ˜¯å¦å„²å­˜çµæœå½±ç‰‡? (y/n, é è¨­:y): ").strip().lower()
        save_output = save != 'n'
        detector.run_video(video_path, save_output=save_output)
    elif choice == '3':
        # è™•ç†å–®å¼µåœ–ç‰‡
        image_path = input("è«‹è¼¸å…¥åœ–ç‰‡è·¯å¾‘: ").strip()
        detector.process_image(image_path, save_output=True)
    elif choice == '4':
        # æ‰¹æ¬¡è™•ç†åœ–ç‰‡
        folder_path = input("è«‹è¼¸å…¥åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘: ").strip()
        detector.process_image_batch(folder_path, save_output=True)
    else:
        print("âŒ ç„¡æ•ˆçš„é¸é …")


if __name__ == "__main__":
    main()
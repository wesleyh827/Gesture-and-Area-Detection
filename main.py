import cv2
from ultralytics import YOLO
import time
import numpy as np
import os

class GestureDetector:
    def __init__(self, model_size='n', alert_duration=3.0, debug=False):
        """
        åˆå§‹åŒ–æ‰‹å‹¢åµæ¸¬å™¨
        
        Args:
            model_size: æ¨¡å‹å¤§å° ('n', 's', 'm', 'l', 'x')
            alert_duration: è­¦ç¤ºå‹•ä½œæŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰
            debug: æ˜¯å¦å•Ÿç”¨èª¿è©¦æ¨¡å¼
        """
        print(f"è¼‰å…¥ YOLOv8-{model_size}-pose æ¨¡å‹...")
        self.model = YOLO(f'yolov8{model_size}-pose.pt')
        self.alert_threshold = alert_duration
        self.alert_start_time = None
        self.is_alerting = False
        self.gui_available = self._check_gui_support()
        self.debug = debug
        
    def _check_gui_support(self):
        """æª¢æŸ¥æ˜¯å¦æ”¯æ´ GUI é¡¯ç¤º"""
        try:
            # å˜—è©¦å‰µå»ºä¸€å€‹æ¸¬è©¦è¦–çª—
            test_img = np.zeros((100, 100, 3), dtype=np.uint8)
            cv2.imshow('test', test_img)
            cv2.destroyAllWindows()
            return True
        except:
            print("âš  åµæ¸¬åˆ°ç³»çµ±ä¸æ”¯æ´ GUI é¡¯ç¤ºï¼Œå°‡åªå„²å­˜çµæœåœ–ç‰‡")
            return False
        
    def check_hands_above_shoulders(self, keypoints, person_id=0):
        """
        æª¢æŸ¥é›™æ‰‹æ˜¯å¦éƒ½åœ¨è‚©è†€ä»¥ä¸Šï¼ˆæˆ–æ¥è¿‘ï¼‰
        
        Args:
            keypoints: [17, 3] é™£åˆ—ï¼ŒåŒ…å« (x, y, confidence)
            person_id: äººå“¡ç·¨è™Ÿï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
        
        Returns:
            bool: é›™æ‰‹æ˜¯å¦åœ¨è‚©è†€ä»¥ä¸Š
        """
        # COCO keypoints ç´¢å¼•
        # 0: é¼»å­, 5: å·¦è‚©, 6: å³è‚©, 7: å·¦æ‰‹è‚˜, 8: å³æ‰‹è‚˜, 9: å·¦æ‰‹è…•, 10: å³æ‰‹è…•
        nose = keypoints[0]
        left_shoulder = keypoints[5]
        right_shoulder = keypoints[6]
        left_elbow = keypoints[7]
        right_elbow = keypoints[8]
        left_wrist = keypoints[9]
        right_wrist = keypoints[10]
        
        # é™ä½ä¿¡å¿ƒåº¦é–¾å€¼ï¼Œè®“åµæ¸¬æ›´å®¹æ˜“
        confidence_threshold = 0.3
        
        # æª¢æŸ¥è‚©è†€æ˜¯å¦è¢«åµæ¸¬åˆ°
        if left_shoulder[2] < confidence_threshold and right_shoulder[2] < confidence_threshold:
            if self.debug:
                print(f"Person #{person_id+1}: âŒ å…©é‚Šè‚©è†€éƒ½ç„¡æ³•åµæ¸¬")
            return False
        
        # è¨ˆç®—è‚©è†€é«˜åº¦ï¼ˆå®¹éŒ¯è™•ç†ï¼šå¦‚æœåªæœ‰ä¸€é‚Šè‚©è†€å¯ç”¨ï¼Œå°±ç”¨é‚£ä¸€é‚Šï¼‰
        if left_shoulder[2] >= confidence_threshold and right_shoulder[2] >= confidence_threshold:
            shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2
            shoulder_info = "é›™è‚©å¹³å‡"
        elif left_shoulder[2] >= confidence_threshold:
            shoulder_y = left_shoulder[1]
            shoulder_info = "åƒ…å·¦è‚©"
        else:
            shoulder_y = right_shoulder[1]
            shoulder_info = "åƒ…å³è‚©"
        
        # å¢åŠ å®¹å·®å€¼ï¼Œè®“åµæ¸¬æ›´å¯¬é¬†
        tolerance = 80  # å¾ 60 å¢åŠ åˆ° 80
        
        # å·¦æ‰‹åˆ¤å®š
        left_hand_up = False
        left_reason = ""
        left_y = None
        
        # å„ªå…ˆé †åºï¼šæ‰‹è…• > æ‰‹è‚˜
        if left_wrist[2] >= confidence_threshold:
            left_y = left_wrist[1]
            left_hand_up = left_y < (shoulder_y + tolerance)
            left_reason = f"æ‰‹è…• y={left_y:.1f}"
        elif left_elbow[2] >= confidence_threshold:
            left_y = left_elbow[1]
            left_hand_up = left_y < (shoulder_y + tolerance * 0.7)
            left_reason = f"æ‰‹è‚˜ y={left_y:.1f}"
        else:
            left_reason = "ç„¡æ³•åµæ¸¬"
        
        # å³æ‰‹åˆ¤å®š
        right_hand_up = False
        right_reason = ""
        right_y = None
        
        if right_wrist[2] >= confidence_threshold:
            right_y = right_wrist[1]
            right_hand_up = right_y < (shoulder_y + tolerance)
            right_reason = f"æ‰‹è…• y={right_y:.1f}"
        elif right_elbow[2] >= confidence_threshold:
            right_y = right_elbow[1]
            right_hand_up = right_y < (shoulder_y + tolerance * 0.7)
            right_reason = f"æ‰‹è‚˜ y={right_y:.1f}"
        else:
            right_reason = "ç„¡æ³•åµæ¸¬"
        
        # å¯¬é¬†åˆ¤å®šï¼šåªè¦åµæ¸¬åˆ°çš„æ‰‹éƒ½èˆ‰èµ·å°±ç®—ï¼ˆå®¹éŒ¯ï¼‰
        # å¦‚æœå…©éš»æ‰‹éƒ½æœ‰åµæ¸¬åˆ°ï¼Œå¿…é ˆéƒ½èˆ‰èµ·
        # å¦‚æœåªæœ‰ä¸€éš»æ‰‹è¢«åµæ¸¬åˆ°ï¼Œé‚£éš»æ‰‹èˆ‰èµ·å°±ç®—
        detected_hands = 0
        raised_hands = 0
        
        if left_y is not None:
            detected_hands += 1
            if left_hand_up:
                raised_hands += 1
        
        if right_y is not None:
            detected_hands += 1
            if right_hand_up:
                raised_hands += 1
        
        # åˆ¤å®šé‚è¼¯ï¼šåµæ¸¬åˆ°çš„æ‰‹éƒ½å¿…é ˆèˆ‰èµ·
        result = detected_hands > 0 and raised_hands == detected_hands
        
        # èª¿è©¦è¼¸å‡º
        if self.debug:
            print(f"\n{'='*60}")
            print(f"Person #{person_id+1} è©³ç´°åˆ†æ:")
            print(f"  é ­éƒ¨ (é¼»å­): {'âœ… åµæ¸¬åˆ°' if nose[2] >= confidence_threshold else 'âŒ æœªåµæ¸¬åˆ°'} (ä¿¡å¿ƒåº¦: {nose[2]:.2f})")
            print(f"  è‚©è†€: {shoulder_info}, é«˜åº¦={shoulder_y:.1f}")
            print(f"  å®¹å·®å€¼: {tolerance} åƒç´ ")
            print(f"  å·¦è‚©: ä¿¡å¿ƒåº¦={left_shoulder[2]:.2f}")
            print(f"  å³è‚©: ä¿¡å¿ƒåº¦={right_shoulder[2]:.2f}")
            print(f"  å·¦æ‰‹: {'âœ… èˆ‰èµ·' if left_hand_up else 'âŒ æœªèˆ‰èµ·'} ({left_reason})")
            print(f"    - æ‰‹è…•ä¿¡å¿ƒåº¦: {left_wrist[2]:.2f}")
            print(f"    - æ‰‹è‚˜ä¿¡å¿ƒåº¦: {left_elbow[2]:.2f}")
            print(f"  å³æ‰‹: {'âœ… èˆ‰èµ·' if right_hand_up else 'âŒ æœªèˆ‰èµ·'} ({right_reason})")
            print(f"    - æ‰‹è…•ä¿¡å¿ƒåº¦: {right_wrist[2]:.2f}")
            print(f"    - æ‰‹è‚˜ä¿¡å¿ƒåº¦: {right_elbow[2]:.2f}")
            print(f"  åµæ¸¬åˆ° {detected_hands} éš»æ‰‹ï¼Œèˆ‰èµ· {raised_hands} éš»")
            print(f"  æœ€çµ‚åˆ¤å®š: {'âœ… è­¦ç¤ºå§¿å‹¢' if result else 'âŒ éè­¦ç¤ºå§¿å‹¢'}")
            print(f"{'='*60}")
        
        return result
    
    def draw_info(self, frame, hands_up, duration=0, is_static=False, person_info=""):
        """åœ¨ç•«é¢ä¸Šç¹ªè£½è³‡è¨Š"""
        # ç¹ªè£½ç‹€æ…‹è³‡è¨Š
        if hands_up:
            if is_static:
                # åœ–ç‰‡æ¨¡å¼ï¼šç›´æ¥é¡¯ç¤ºè­¦ç¤ºï¼ˆä¸éœ€è¦ç­‰3ç§’ï¼‰
                cv2.rectangle(frame, (0, 0), (frame.shape[1], 100), (0, 0, 255), -1)
                cv2.putText(frame, f"WARNING: ALERT POSE DETECTED!{person_info}", (50, 60),
                          cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 4)
            elif duration >= self.alert_threshold:
                # å½±ç‰‡æ¨¡å¼ï¼šé”åˆ°3ç§’æ‰é¡¯ç¤ºè­¦ç¤º
                cv2.rectangle(frame, (0, 0), (frame.shape[1], 100), (0, 0, 255), -1)
                cv2.putText(frame, f"WARNING: ALERT DETECTED!{person_info}", (50, 60),
                          cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 4)
            else:
                # å½±ç‰‡æ¨¡å¼ï¼šåµæ¸¬åˆ°èˆ‰æ‰‹ä½†æœªé”3ç§’ï¼Œé¡¯ç¤ºå€’æ•¸è¨ˆæ™‚
                progress = (duration / self.alert_threshold) * 100
                cv2.rectangle(frame, (0, 0), (frame.shape[1], 80), (0, 165, 255), -1)
                cv2.putText(frame, f"Hands Up{person_info}: {duration:.1f}s / {self.alert_threshold:.1f}s ({progress:.0f}%)", 
                          (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
        else:
            # æœªåµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢
            if is_static:
                cv2.putText(frame, "No Alert Pose Detected", (20, 40),
                          cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        return frame
    
    def process_frame(self, frame, is_static=False):
        """
        è™•ç†å–®ä¸€å½±æ ¼
        
        Args:
            frame: è¼¸å…¥å½±æ ¼
            is_static: æ˜¯å¦ç‚ºéœæ…‹åœ–ç‰‡ï¼ˆä¸éœ€è¨ˆæ™‚ï¼‰
        
        Returns:
            annotated_frame: æ¨™è¨»å¾Œçš„å½±æ ¼
            hands_up: æ˜¯å¦åµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢
        """
        # åŸ·è¡Œå§¿æ…‹ä¼°è¨ˆ
        results = self.model(frame, verbose=False)
        
        hands_up = False
        duration = 0
        alert_person_id = -1
        person_statuses = []  # è¨˜éŒ„æ¯å€‹äººçš„ç‹€æ…‹
        
        # æª¢æŸ¥æ˜¯å¦æœ‰åµæ¸¬åˆ°äºº
        if results[0].keypoints is not None and len(results[0].keypoints.data) > 0:
            # æª¢æŸ¥æ‰€æœ‰åµæ¸¬åˆ°çš„äºº
            for person_id, person_keypoints in enumerate(results[0].keypoints.data):
                # æª¢æŸ¥é€™å€‹äººæ˜¯å¦é›™æ‰‹èˆ‰èµ·
                is_alert = self.check_hands_above_shoulders(person_keypoints, person_id)
                person_statuses.append({
                    'id': person_id,
                    'is_alert': is_alert,
                    'keypoints': person_keypoints
                })
                
                if is_alert:
                    hands_up = True
                    alert_person_id = person_id
                    
                    if not is_static:
                        if self.alert_start_time is None:
                            self.alert_start_time = time.time()
                        
                        duration = time.time() - self.alert_start_time
                        
                        # æª¢æŸ¥æ˜¯å¦é”åˆ°è­¦ç¤ºæ™‚é–“
                        if duration >= self.alert_threshold and not self.is_alerting:
                            self.is_alerting = True
                            print(f"\nğŸš¨ è­¦ç¤ºè§¸ç™¼ï¼äººå“¡ #{person_id+1}ï¼ŒæŒçºŒæ™‚é–“: {duration:.2f}ç§’")
                    
                    # æ‰¾åˆ°ä¸€å€‹èˆ‰æ‰‹çš„äººå°±å¤ äº†ï¼ˆå¦‚æœéœ€è¦æª¢æŸ¥æ‰€æœ‰äººï¼Œç§»é™¤é€™å€‹ breakï¼‰
                    break
            
            # å¦‚æœæ²’æœ‰äººèˆ‰æ‰‹ï¼Œé‡ç½®è¨ˆæ™‚å™¨
            if not hands_up:
                if not is_static and self.alert_start_time is not None:
                    print(f"é‡ç½®è¨ˆæ™‚å™¨ï¼ˆæŒçºŒäº† {time.time() - self.alert_start_time:.2f}ç§’ï¼‰")
                self.alert_start_time = None
                self.is_alerting = False
        
        # ç¹ªè£½å§¿æ…‹éª¨æ¶
        annotated_frame = results[0].plot()
        
        # åœ¨æ¯å€‹äººèº«ä¸Šæ¨™è¨»ç·¨è™Ÿå’Œç‹€æ…‹ï¼ˆå‚³é duration å’Œ is_staticï¼‰
        annotated_frame = self.draw_person_labels(annotated_frame, person_statuses, results[0], duration, is_static)
        
        # ç¹ªè£½é ‚éƒ¨è³‡è¨Šæ¢
        info_text = f" (Person #{alert_person_id+1})" if alert_person_id >= 0 else ""
        annotated_frame = self.draw_info(annotated_frame, hands_up, duration, is_static, info_text)
        
        return annotated_frame, hands_up
    
    def draw_person_labels(self, frame, person_statuses, result, duration=0, is_static=False):
        """åœ¨æ¯å€‹äººèº«ä¸Šç¹ªè£½ç·¨è™Ÿå’Œç‹€æ…‹æ¨™ç±¤"""
        # å–å¾— bounding boxesï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        if result.boxes is not None and len(result.boxes) > 0:
            boxes = result.boxes.xyxy.cpu().numpy()
        else:
            boxes = None
        
        for person_status in person_statuses:
            person_id = person_status['id']
            is_alert = person_status['is_alert']
            keypoints = person_status['keypoints']
            
            # è¨ˆç®—äººé«”çš„é‚Šç•Œæ¡†ï¼ˆå¾é—œéµé»ï¼‰
            valid_keypoints = keypoints[keypoints[:, 2] > 0.3]  # åªå–ä¿¡å¿ƒåº¦ > 0.3 çš„é»
            
            if len(valid_keypoints) > 0:
                x_min = int(valid_keypoints[:, 0].min())
                y_min = int(valid_keypoints[:, 1].min())
                x_max = int(valid_keypoints[:, 0].max())
                y_max = int(valid_keypoints[:, 1].max())
                
                # åˆ¤å®šæ˜¯å¦è¦é¡¯ç¤ºè­¦ç¤ºæ¨™è¨»
                # åœ–ç‰‡æ¨¡å¼ï¼šåªè¦èˆ‰æ‰‹å°±é¡¯ç¤ºè­¦ç¤º
                # å½±ç‰‡æ¨¡å¼ï¼šå¿…é ˆæŒçºŒ3ç§’æ‰é¡¯ç¤ºè­¦ç¤º
                show_alert = is_alert and (is_static or duration >= self.alert_threshold)
                show_warning = is_alert and not is_static and duration < self.alert_threshold
                
                # è¨­å®šé¡è‰²
                if show_alert:
                    color = (0, 0, 255)  # ç´…è‰²ï¼šçœŸæ­£çš„è­¦ç¤º
                    label_bg_color = (0, 0, 200)
                elif show_warning:
                    color = (0, 165, 255)  # æ©˜è‰²ï¼šè­¦å‘Šä¸­ï¼ˆæœªé”3ç§’ï¼‰
                    label_bg_color = (0, 140, 220)
                else:
                    color = (0, 255, 0)  # ç¶ è‰²ï¼šæ­£å¸¸
                    label_bg_color = (0, 200, 0)
                
                # ç¹ªè£½é‚Šæ¡†
                thickness = 3 if show_alert else 2
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, thickness)
                
                # æº–å‚™æ¨™ç±¤æ–‡å­—
                label = f"Person #{person_id+1}"
                if show_alert:
                    label += " [ALERT!]"
                elif show_warning:
                    label += f" [Warning {duration:.1f}s]"
                
                # è¨ˆç®—æ–‡å­—å¤§å°
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 0.7
                font_thickness = 2
                (text_width, text_height), baseline = cv2.getTextSize(
                    label, font, font_scale, font_thickness
                )
                
                # ç¹ªè£½æ¨™ç±¤èƒŒæ™¯
                label_y = y_min - 10
                if label_y < text_height + 10:
                    label_y = y_min + text_height + 10
                
                cv2.rectangle(
                    frame,
                    (x_min, label_y - text_height - 10),
                    (x_min + text_width + 10, label_y + 5),
                    label_bg_color,
                    -1
                )
                
                # ç¹ªè£½æ¨™ç±¤æ–‡å­—
                cv2.putText(
                    frame,
                    label,
                    (x_min + 5, label_y - 5),
                    font,
                    font_scale,
                    (255, 255, 255),
                    font_thickness
                )
                
                # åªæœ‰çœŸæ­£é”åˆ°è­¦ç¤ºæ™‚é–“æ‰ç•«è­¦ç¤ºç¬¦è™Ÿ
                if show_alert:
                    center_x = (x_min + x_max) // 2
                    center_y = (y_min + y_max) // 2
                    
                    # ç•«ä¸€å€‹è­¦ç¤ºä¸‰è§’å½¢
                    triangle_size = 40
                    pts = np.array([
                        [center_x, center_y - triangle_size],
                        [center_x - triangle_size, center_y + triangle_size],
                        [center_x + triangle_size, center_y + triangle_size]
                    ], np.int32)
                    
                    cv2.fillPoly(frame, [pts], (0, 0, 255))
                    cv2.polylines(frame, [pts], True, (255, 255, 255), 3)
                    
                    # åœ¨ä¸‰è§’å½¢ä¸­é–“ç•«é©šå˜†è™Ÿ
                    cv2.putText(
                        frame,
                        "!",
                        (center_x - 8, center_y + 15),
                        cv2.FONT_HERSHEY_SIMPLEX,
                        1.5,
                        (255, 255, 255),
                        3
                    )
        
        return frame
    
    def process_image(self, image_path, save_output=True, output_dir='output'):
        """
        è™•ç†å–®å¼µåœ–ç‰‡
        
        Args:
            image_path: åœ–ç‰‡è·¯å¾‘
            save_output: æ˜¯å¦å„²å­˜çµæœ
            output_dir: è¼¸å‡ºè³‡æ–™å¤¾
        
        Returns:
            bool: æ˜¯å¦åµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢
        """
        print(f"\nè™•ç†åœ–ç‰‡: {image_path}")
        
        # è®€å–åœ–ç‰‡
        if not os.path.exists(image_path):
            print(f"âŒ æ‰¾ä¸åˆ°åœ–ç‰‡: {image_path}")
            return False
        
        frame = cv2.imread(image_path)
        if frame is None:
            print(f"âŒ ç„¡æ³•è®€å–åœ–ç‰‡: {image_path}")
            return False
        
        # è™•ç†åœ–ç‰‡ï¼ˆéœæ…‹æ¨¡å¼ï¼‰
        annotated_frame, hands_up = self.process_frame(frame, is_static=True)
        
        # é¡¯ç¤ºçµæœ
        print(f"{'âœ… åµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢ï¼' if hands_up else 'âŒ æœªåµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢'}")
        
        # å„²å­˜çµæœ
        output_path = None
        if save_output:
            os.makedirs(output_dir, exist_ok=True)
            filename = os.path.basename(image_path)
            name, ext = os.path.splitext(filename)
            output_path = os.path.join(output_dir, f"{name}_result{ext}")
            cv2.imwrite(output_path, annotated_frame)
            print(f"ğŸ’¾ çµæœå·²å„²å­˜è‡³: {output_path}")
        
        # åªåœ¨æ”¯æ´ GUI æ™‚é¡¯ç¤ºåœ–ç‰‡
        if self.gui_available:
            try:
                cv2.imshow('Gesture Detection - Image', annotated_frame)
                print("æŒ‰ä»»æ„éµç¹¼çºŒ...")
                cv2.waitKey(0)
                cv2.destroyAllWindows()
            except Exception as e:
                print(f"âš  ç„¡æ³•é¡¯ç¤ºåœ–ç‰‡: {e}")
        else:
            print(f"ğŸ’¡ è«‹é–‹å•Ÿ {output_path} æŸ¥çœ‹çµæœ")
        
        return hands_up
    
    def process_image_batch(self, image_folder, save_output=True, output_dir='output'):
        """
        æ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡
        
        Args:
            image_folder: åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘
            save_output: æ˜¯å¦å„²å­˜çµæœ
            output_dir: è¼¸å‡ºè³‡æ–™å¤¾
        """
        print(f"\næ‰¹æ¬¡è™•ç†è³‡æ–™å¤¾: {image_folder}")
        
        if not os.path.exists(image_folder):
            print(f"âŒ æ‰¾ä¸åˆ°è³‡æ–™å¤¾: {image_folder}")
            return
        
        # æ”¯æ´çš„åœ–ç‰‡æ ¼å¼
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.webp']
        
        # å–å¾—æ‰€æœ‰åœ–ç‰‡
        image_files = []
        for file in os.listdir(image_folder):
            if any(file.lower().endswith(ext) for ext in image_extensions):
                image_files.append(os.path.join(image_folder, file))
        
        if not image_files:
            print("âŒ è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ°åœ–ç‰‡æª”æ¡ˆ")
            return
        
        print(f"æ‰¾åˆ° {len(image_files)} å¼µåœ–ç‰‡\n")
        
        # çµ±è¨ˆçµæœ
        alert_count = 0
        
        # è™•ç†æ¯å¼µåœ–ç‰‡
        for i, image_path in enumerate(image_files, 1):
            print(f"[{i}/{len(image_files)}] ", end="")
            
            frame = cv2.imread(image_path)
            if frame is None:
                print(f"âš  è·³éç„¡æ³•è®€å–çš„åœ–ç‰‡: {image_path}")
                continue
            
            # è™•ç†åœ–ç‰‡
            annotated_frame, hands_up = self.process_frame(frame, is_static=True)
            
            if hands_up:
                alert_count += 1
                print(f"âœ… {os.path.basename(image_path)} - åµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢")
            else:
                print(f"âŒ {os.path.basename(image_path)} - æœªåµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢")
            
            # å„²å­˜çµæœ
            if save_output:
                os.makedirs(output_dir, exist_ok=True)
                filename = os.path.basename(image_path)
                name, ext = os.path.splitext(filename)
                output_path = os.path.join(output_dir, f"{name}_result{ext}")
                cv2.imwrite(output_path, annotated_frame)
        
        # é¡¯ç¤ºçµ±è¨ˆ
        print(f"\n{'='*50}")
        print(f"è™•ç†å®Œæˆï¼")
        print(f"ç¸½åœ–ç‰‡æ•¸: {len(image_files)}")
        print(f"åµæ¸¬åˆ°è­¦ç¤ºå§¿å‹¢: {alert_count} å¼µ")
        print(f"æœªåµæ¸¬åˆ°: {len(image_files) - alert_count} å¼µ")
        if save_output:
            print(f"ğŸ’¾ çµæœå·²å„²å­˜è‡³: {output_dir}/")
        print(f"{'='*50}")
    
    def run_webcam(self, camera_id=0):
        """ä½¿ç”¨æ”å½±æ©Ÿå³æ™‚åµæ¸¬"""
        if not self.gui_available:
            print("âŒ ç³»çµ±ä¸æ”¯æ´ GUI é¡¯ç¤ºï¼Œç„¡æ³•ä½¿ç”¨æ”å½±æ©Ÿæ¨¡å¼")
            print("ğŸ’¡ è«‹ä½¿ç”¨åœ–ç‰‡è™•ç†æ¨¡å¼")
            return
            
        print(f"\né–‹å§‹ä½¿ç”¨æ”å½±æ©Ÿ {camera_id}")
        print("è«‹é›™æ‰‹èˆ‰éè‚©è†€3ç§’ä»¥ä¸Šä¾†æ¸¬è©¦")
        print("æŒ‰ 'q' é›¢é–‹\n")
        
        cap = cv2.VideoCapture(camera_id)
        
        if not cap.isOpened():
            print(f"âŒ ç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿ {camera_id}")
            return
        
        # è¨­å®šè§£æåº¦ï¼ˆå¯é¸ï¼‰
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                print("ç„¡æ³•è®€å–å½±åƒ")
                break
            
            # è™•ç†å½±æ ¼
            processed_frame, _ = self.process_frame(frame, is_static=False)
            
            # é¡¯ç¤ºçµæœ
            cv2.imshow('Gesture Detection - Webcam', processed_frame)
            
            # æŒ‰ 'q' é›¢é–‹
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()
        print("ç¨‹å¼çµæŸ")
    
    def run_video(self, video_path, save_output=True, output_dir='output'):
        """
        è™•ç†å½±ç‰‡æª”æ¡ˆ
        
        Args:
            video_path: å½±ç‰‡è·¯å¾‘
            save_output: æ˜¯å¦å„²å­˜çµæœå½±ç‰‡
            output_dir: è¼¸å‡ºè³‡æ–™å¤¾
        """
        print(f"\nè™•ç†å½±ç‰‡: {video_path}")
        
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡ {video_path}")
            return
        
        # å–å¾—å½±ç‰‡è³‡è¨Š
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"å½±ç‰‡è³‡è¨Š: {width}x{height} @ {fps}fps, ç¸½å½±æ ¼æ•¸: {total_frames}")
        
        # æº–å‚™è¼¸å‡ºå½±ç‰‡
        out = None
        output_path = None
        if save_output:
            os.makedirs(output_dir, exist_ok=True)
            filename = os.path.basename(video_path)
            name, ext = os.path.splitext(filename)
            output_path = os.path.join(output_dir, f"{name}_result{ext}")
            
            # ä½¿ç”¨ mp4v ç·¨ç¢¼å™¨ï¼ˆç›¸å®¹æ€§è¼ƒå¥½ï¼‰
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            print(f"ğŸ’¾ å°‡å„²å­˜çµæœè‡³: {output_path}")
        
        # è¨˜éŒ„è­¦ç¤ºäº‹ä»¶
        alert_events = []
        frame_count = 0
        
        print("\né–‹å§‹è™•ç†å½±ç‰‡...")
        if self.gui_available:
            print("æŒ‰ 'q' æå‰çµæŸï¼ŒæŒ‰ 'p' æš«åœ")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # è™•ç†å½±æ ¼
            processed_frame, hands_up = self.process_frame(frame, is_static=False)
            
            # è¨˜éŒ„è­¦ç¤ºäº‹ä»¶
            if self.is_alerting and (not alert_events or not alert_events[-1]['end']):
                if not alert_events or alert_events[-1]['end']:
                    # æ–°çš„è­¦ç¤ºäº‹ä»¶
                    alert_events.append({
                        'start_frame': frame_count,
                        'start_time': frame_count / fps,
                        'end': False
                    })
            elif not self.is_alerting and alert_events and not alert_events[-1]['end']:
                # è­¦ç¤ºçµæŸ
                alert_events[-1]['end_frame'] = frame_count
                alert_events[-1]['end_time'] = frame_count / fps
                alert_events[-1]['end'] = True
            
            # å„²å­˜å½±æ ¼
            if out is not None:
                out.write(processed_frame)
            
            # é¡¯ç¤ºé€²åº¦
            if frame_count % 30 == 0:  # æ¯30å¹€é¡¯ç¤ºä¸€æ¬¡
                progress = (frame_count / total_frames) * 100
                print(f"\rè™•ç†ä¸­... {progress:.1f}% ({frame_count}/{total_frames})", end='')
            
            # é¡¯ç¤ºçµæœï¼ˆå¦‚æœæ”¯æ´GUIï¼‰
            if self.gui_available:
                cv2.imshow('Gesture Detection - Video', processed_frame)
                
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("\n\nâš  ä½¿ç”¨è€…ä¸­æ–·è™•ç†")
                    break
                elif key == ord('p'):
                    print("\næš«åœä¸­ï¼ŒæŒ‰ä»»æ„éµç¹¼çºŒ...")
                    cv2.waitKey(0)
        
        print("\n\nå½±ç‰‡è™•ç†å®Œæˆï¼")
        
        # é‡‹æ”¾è³‡æº
        cap.release()
        if out is not None:
            out.release()
        if self.gui_available:
            cv2.destroyAllWindows()
        
        # é¡¯ç¤ºçµæœçµ±è¨ˆ
        print(f"\n{'='*50}")
        print(f"è™•ç†çµ±è¨ˆ:")
        print(f"ç¸½å½±æ ¼æ•¸: {frame_count}")
        print(f"å½±ç‰‡é•·åº¦: {frame_count/fps:.2f} ç§’")
        print(f"åµæ¸¬åˆ°è­¦ç¤ºäº‹ä»¶: {len(alert_events)} æ¬¡")
        
        if alert_events:
            print(f"\nè­¦ç¤ºäº‹ä»¶è©³æƒ…:")
            for i, event in enumerate(alert_events, 1):
                start_time = event['start_time']
                end_time = event.get('end_time', frame_count / fps)
                duration = end_time - start_time
                print(f"  äº‹ä»¶ {i}: {start_time:.2f}s - {end_time:.2f}s (æŒçºŒ {duration:.2f}s)")
        
        if save_output and output_path:
            print(f"\nğŸ’¾ çµæœå½±ç‰‡å·²å„²å­˜è‡³: {output_path}")
        
        print(f"{'='*50}\n")


def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 50)
    print("æ‰‹å‹¢è­¦ç¤ºåµæ¸¬ç³»çµ±")
    print("=" * 50)
    
    # å»ºç«‹åµæ¸¬å™¨ï¼ˆå•Ÿç”¨èª¿è©¦æ¨¡å¼ï¼‰
    detector = GestureDetector(
        model_size='n',  # ä½¿ç”¨ nano æ¨¡å‹ï¼ˆæœ€å¿«ï¼‰
        alert_duration=3.0,  # 3ç§’è­¦ç¤º
        debug=True  # å•Ÿç”¨èª¿è©¦æ¨¡å¼ï¼Œé¡¯ç¤ºè©³ç´°è³‡è¨Š
    )
    
    # é¸æ“‡æ¨¡å¼
    print("\nè«‹é¸æ“‡æ¨¡å¼:")
    print("1. ä½¿ç”¨æ”å½±æ©Ÿå³æ™‚åµæ¸¬")
    print("2. è™•ç†å½±ç‰‡æª”æ¡ˆ")
    print("3. è™•ç†å–®å¼µåœ–ç‰‡")
    print("4. æ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡")
    
    choice = input("\nè«‹è¼¸å…¥é¸é … (1-4): ").strip()
    
    if choice == '1':
        # ä½¿ç”¨æ”å½±æ©Ÿ
        detector.run_webcam(camera_id=0)
    elif choice == '2':
        # ä½¿ç”¨å½±ç‰‡æª”æ¡ˆ
        video_path = input("è«‹è¼¸å…¥å½±ç‰‡è·¯å¾‘: ").strip()
        save = input("æ˜¯å¦å„²å­˜çµæœå½±ç‰‡? (y/n, é è¨­:y): ").strip().lower()
        save_output = save != 'n'
        detector.run_video(video_path, save_output=save_output)
    elif choice == '3':
        # è™•ç†å–®å¼µåœ–ç‰‡
        image_path = input("è«‹è¼¸å…¥åœ–ç‰‡è·¯å¾‘: ").strip()
        detector.process_image(image_path, save_output=True)
    elif choice == '4':
        # æ‰¹æ¬¡è™•ç†åœ–ç‰‡
        folder_path = input("è«‹è¼¸å…¥åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘: ").strip()
        detector.process_image_batch(folder_path, save_output=True)
    else:
        print("âŒ ç„¡æ•ˆçš„é¸é …")


if __name__ == "__main__":
    main()
